{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Byte Pair Encoding (BPE) is a text compression technique that iteratively replaces the most frequent pair of bytes in a sequence with a single, unused byte. Adapted for natural language processing, BPE is used in tokenization, where it iteratively merges the most frequent pair of adjacent characters or character sequences (tokens) into a single new token. This approach allows handling of common words as single entities and rare words or names to be broken down into smaller pieces, improving the model's ability to understand and generate text by efficiently managing vocabulary size and out-of-vocabulary words."
      ],
      "metadata": {
        "id": "VD_piaBH4cc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djDDgFG-246c",
        "outputId": "81bdabfd-0319-447e-92fa-377c14bfaf61"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uuvR6ZGW2W9Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import regex as re\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download the tiny shakespeare dataset\n",
        "data_dir = os.path.join('data', 'tinyshakespeare')\n",
        "input_file_path = os.path.join(data_dir, 'input.txt')\n",
        "if not os.path.exists(input_file_path):\n",
        "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    os.makedirs(data_dir)\n",
        "    with open(input_file_path, 'w') as f:\n",
        "        f.write(requests.get(data_url).text)\n",
        "\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "n = len(data)\n",
        "print(\"n\",n,\" \",(n*0.9))\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "print(\"train_data\",train_data[0:30])\n",
        "# encode with tiktoken gpt2 bpe\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "print(f\"val has {len(val_ids):,} tokens\")\n",
        "\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.uint16)\n",
        "val_ids = np.array(val_ids, dtype=np.uint16)\n",
        "train_ids.tofile(os.path.join(data_dir, 'train.bin'))\n",
        "val_ids.tofile(os.path.join(data_dir, 'val.bin'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN5QVMhk23SE",
        "outputId": "c65189dc-3b2d-40db-9809-4e48226178b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n 1115394   1003854.6\n",
            "train_data First Citizen:\n",
            "Before we proce\n",
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "class CustomConfig(GPTConfig):\n",
        "    # model\n",
        "    n_layer = 5 #increase number of decoder layer more than 1\n",
        "    n_head = 8\n",
        "    n_embd = 256\n",
        "    embd_pdrop = 0.1\n",
        "    resid_pdrop = 0.1\n",
        "    attn_pdrop = 0.1\n",
        "    dropout = 0.1\n",
        "    compile = True\n",
        "\n",
        "    # data\n",
        "    device = 'cuda'\n",
        "    num_workers = 0\n",
        "\n",
        "    # optimizer parameters\n",
        "    max_iters = 2e4\n",
        "    batch_size = 4\n",
        "    block_size = 64\n",
        "    learning_rate = 6e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    weight_decay = 1e-1\n",
        "    grad_norm_clip = 1.0\n",
        "\n",
        "# config\n",
        "vocab_size = len(train_ids)\n",
        "config = CustomConfig(vocab_size=vocab_size)"
      ],
      "metadata": {
        "id": "yDouXINk23Ud"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join('data', 'tinyshakespeare')\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "class ShakespeareDataset(Dataset):\n",
        "    def __init__(self, split, block_size=128, device_type='cuda'):\n",
        "        assert split in {'train', 'test'}\n",
        "        self.split = split\n",
        "        self.block_size = block_size\n",
        "        self.device_type = device_type\n",
        "        self.data = train_data if split == 'train' else val_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "        # x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "        # y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "        x = torch.from_numpy(self.data[idx : idx + self.block_size].astype(np.int64))\n",
        "        y = torch.from_numpy(self.data[idx + 1 : idx + 1 + self.block_size].astype(np.int64))\n",
        "\n",
        "\n",
        "        x, y = x.to('cpu'), y.to('cpu')\n",
        "        return x, y\n",
        "\n",
        "train_dataset = ShakespeareDataset('train', config.block_size, config.device)\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)\n",
        "test_dataset = ShakespeareDataset('test', config.block_size, config.device)\n",
        "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, num_workers=config.num_workers, drop_last=False)\n",
        "sample_data = next(iter(train_loader))\n",
        "x, y = sample_data\n",
        "print(\"x:\", x.size())\n",
        "print(\"y:\", y.size())\n",
        "print(\"X\",x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0o9p4wH5h1q",
        "outputId": "f61ba7f5-a38c-4bba-fcb2-72a7a68feb2e"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: torch.Size([4, 64])\n",
            "y: torch.Size([4, 64])\n",
            "X tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
            "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
            "          461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
            "         1639,   389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,\n",
            "          680,    30,   198,   198,  3237,    25,   198,  4965,  5634,    13,\n",
            "        12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,    11,\n",
            "          345,   760,   327,  1872])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NewGELU(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT).\n",
        "    Reference: Gaussian Error Linear Units (GELU) paper: https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It's important in decoder block to have diagonal mask\n",
        "    It is also possible to use torch.nn.MultiheadAttention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        ''' The assert statement checks that the model's embedding dimension (config.n_embd) is\n",
        "         divisible by the number of attention heads (config.n_head). This is crucial because, in\n",
        "         multi-head attention mechanisms, the embedding dimension must be evenly split among the heads for parallel processing.\n",
        "         If this condition is not met, the assert statement raises an AssertionError, halting execution.\n",
        "        This ensures the model configuration is valid for the intended architecture.'''\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd) #256*(3*256)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)  #256*256\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
        "        self.dropout = config.dropout\n",
        "        self.n_head = config.n_head  #8\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "        # flash attention make GPU go faster but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            # use register_buffer when: you want a stateful part of your model that is not a parameter\n",
        "            # but you want it in your state_dict\n",
        "            self.register_buffer(\n",
        "                \"mask\",\n",
        "                torch.tril(torch.ones(config.block_size, config.block_size)\n",
        "            ).view(1, 1, config.block_size, config.block_size))\n",
        "            \"\"\" If the block_size is 64, the output tensor's size after applying\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size) will be [1, 1, 64, 64].\n",
        "             This represents a 4-dimensional tensor where the lower triangular matrix of size 64x64 is reshaped to have two additional leading dimensions\n",
        "             of size 1, primarily for batch and head dimensions compatibility in attention mechanisms.\"\"\"\n",
        "    def forward(self, x):\n",
        "        # batch_size, seq_len, emb_dim\n",
        "        B, T, C = x.size()\n",
        "        print(\"B, T, C\",x.size(),B) #4*64*256\n",
        "        print(\"self.c_attn(x)\",self.c_attn(x).size()) #4*64*768\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # (b, seq_len, emb_dim) --> (b, seq_len, emb_dim * 3) --> (b, seq_len, emb_dim)\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        print(k.size())\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (b, h, seq_len, d_k)\n",
        "        print(\"K Size\",k.size(),k.shape)\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        \"\"\"Causal self-attention is a mechanism in transformers that restricts the model from attending to future tokens in the input sequence,\n",
        "        ensuring each position can only be influenced by itself and preceding positions. This is crucial for generating coherent sequences in tasks like language modeling, where the prediction for a current token must\n",
        "         not depend on future tokens, maintaining the sequence's causal structure. \"\"\"\n",
        "\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(\n",
        "                q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True\n",
        "            )\n",
        "        else:\n",
        "            # (b, h, seq_len, d_k) matmul (b, h, d_k, seq_len) --> (b, h, seq_len, seq_len)\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            print(\"ATT\",att)\n",
        "            # diagonal mask\n",
        "            # fill 0 mask with super small number so it wont affect the softmax weight\n",
        "            # (batch_size, h, seq_len, seq_len)\n",
        "            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "            print(\"ATT_maskes\",att)\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "\n",
        "            # (b, h, seq_len, seq_len) matmul (b, h, seq_len, d_k) --> (b, h, seq_len, d_k)\n",
        "            y = att @ v\n",
        "\n",
        "        # (b, h, seq_len, d_k) --> (b, seq_len, h, d_k) --> (b, seq_len, d_model)\n",
        "        print(\"Y-Before linear\",y.size())#4*8*64*32\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)#4*64*8*32\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        print(\"Y\",y.size())\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "B44iL8zb5h33"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" GPT only contain decode block\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "\n",
        "\n",
        "        self.mlp = nn.ModuleDict(dict(\n",
        "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
        "            act     = NewGELU(),\n",
        "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
        "            dropout = nn.Dropout(config.resid_pdrop),\n",
        "        ))\n",
        "        m = self.mlp\n",
        "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # (batch_size, seq_len, emb_dim)\n",
        "        print(\"ln_1\",self.ln_1(x).size())\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        print(\"Middle x\",x.size())\n",
        "        print(\"MLPF\",self.mlpf(self.ln_2(x)).size())\n",
        "        x = x + self.mlpf(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "### testing\n",
        "wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "block = Block(config)\n",
        "\n",
        "tok_emb = wte(x)\n",
        "print('Token Embedding Size:', tok_emb.size())\n",
        "\n",
        "block_out = block(tok_emb)\n",
        "print('Block Output Size:', block_out.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-PY7K8w5h7H",
        "outputId": "f73bf0ef-5e76-4fce-c926-e6eff5f81091"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token Embedding Size: torch.Size([4, 64, 256])\n",
            "ln_1 torch.Size([4, 64, 256])\n",
            "B, T, C torch.Size([4, 64, 256]) 4\n",
            "self.c_attn(x) torch.Size([4, 64, 768])\n",
            "torch.Size([4, 64, 256])\n",
            "K Size torch.Size([4, 8, 64, 32]) torch.Size([4, 8, 64, 32])\n",
            "Y-Before linear torch.Size([4, 8, 64, 32])\n",
            "Y torch.Size([4, 64, 256])\n",
            "Middle x torch.Size([4, 64, 256])\n",
            "MLPF torch.Size([4, 64, 256])\n",
            "Block Output Size: torch.Size([4, 64, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch._dynamo\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "class GPT(nn.Module):\n",
        "    \"\"\" GPT Language Model \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "        self.positional_encoding = self._get_positional_encoding(config.block_size, config.n_embd)\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            # wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.embd_pdrop),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
        "        self.apply(self._init_weights)\n",
        "        print(\"named_parameters\",self.named_parameters)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def _get_positional_encoding(self, max_seq_len, embed_size):\n",
        "        positional_encoding = torch.zeros(max_seq_len, embed_size)\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * -(math.log(10000.0) / embed_size))\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        return positional_encoding\n",
        "\n",
        "    def configure_optimizers(self, train_config):\n",
        "\n",
        "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
        "        decay = set()\n",
        "        no_decay = set()\n",
        "        whitelist_weight_modules = (torch.nn.Linear, )\n",
        "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
        "        for mn, m in self.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
        "                # random note: because named_modules and named_parameters are recursive\n",
        "                # we will see the same tensors p many many times. but doing it this way\n",
        "                # allows us to know which parent module any tensor p belongs to...\n",
        "                if pn.endswith('bias'):\n",
        "                    # all biases will not be decayed\n",
        "                    no_decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
        "                    # weights of whitelist modules will be weight decayed\n",
        "                    decay.add(fpn)\n",
        "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
        "                    # weights of blacklist modules will NOT be weight decayed\n",
        "                    no_decay.add(fpn)\n",
        "        print(\"Pn\",pn)\n",
        "        print(\"decay\",decay)\n",
        "        # validate that we considered every parameter\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        inter_params = decay & no_decay\n",
        "        union_params = decay | no_decay\n",
        "\n",
        "        # create the pytorch optimizer object\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
        "        return optimizer\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        tok_emb = self.transformer.wte(idx) # (b, t, n_embd)\n",
        "        pos_emb = self.positional_encoding[:t, :].unsqueeze(0) # (1, t, n_embd)\n",
        "\n",
        "        # positional token, shape (1, t)\n",
        "        # pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)\n",
        "        # pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t, n_embd)\n",
        "\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.transformer.ln_f(x)\n",
        "        # (b, t, n_embd) -- > # (b, t, vocab_size)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        # -1 at output will be ignored\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b, t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
        "\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            print(\"LOGITS\",logits)\n",
        "            logits = torch.exp(logits) / torch.sum(np.exp(logits), axis=-1, keepdims=True)\n",
        "\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                # from [0.2769, 0.1019, 0.3351, 0.1967, 0.0882] --> [0.452, 0.547]\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            print(\"Logits2\",logits)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            print(\"probs\",probs,probs.shape)\n",
        "            if do_sample:\n",
        "                # select from [0.1, 0.2, 0.3, 0.2, 0.2] --> [2, 4] (if num_samples=2)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                # take the top probs index\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            print(\"IDX\",idx)  #keep adding the idx value\n",
        "        return idx\n",
        "\n",
        "### testing\n",
        "wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
        "model = GPT(config)\n",
        "model = torch.compile(model)\n",
        "\n",
        "# sample dataset from data loader\n",
        "logits, loss = model.forward(x, y)\n",
        "print('logits: ', logits.size())\n",
        "print('loss: ', loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uDynzScFP21",
        "outputId": "97b09c4f-031c-474c-95aa-58ebfe4da8be"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "named_parameters <bound method Module.named_parameters of GPT(\n",
            "  (transformer): ModuleDict(\n",
            "    (wte): Embedding(301966, 256)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0): Block(\n",
            "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CausalSelfAttention(\n",
            "          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n",
            "          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): ModuleDict(\n",
            "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
            "          (act): NewGELU()\n",
            "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=256, out_features=301966, bias=False)\n",
            ")>\n",
            "number of parameters: 78.09M\n",
            "ln_1 torch.Size([4, 64, 256])\n",
            "B, T, C torch.Size([4, 64, 256]) 4\n",
            "self.c_attn(x) torch.Size([4, 64, 768])\n",
            "torch.Size([4, 64, 256])\n",
            "K Size torch.Size([4, 8, 64, 32]) (4, 8, 64, 32)\n",
            "Y-Before linear torch.Size([4, 8, 64, 32])\n",
            "Y torch.Size([4, 64, 256])\n",
            "Middle x torch.Size([4, 64, 256])\n",
            "MLPF torch.Size([4, 64, 256])\n",
            "logits:  torch.Size([4, 64, 301966])\n",
            "loss:  tensor(12.7408, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "\n",
        "    def __init__(self, config, model, train_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "        self.callbacks = defaultdict(list)\n",
        "        self.device = config.device\n",
        "        self.model = self.model\n",
        "\n",
        "        # variables that will be assigned to trainer class later for logging and etc\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "\n",
        "        # setup the optimizer\n",
        "        self.optimizer = model.configure_optimizers(config)\n",
        "\n",
        "        # setup the dataloader\n",
        "        train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            sampler=torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n",
        "            shuffle=False,\n",
        "            # pin_memory=True,\n",
        "            batch_size=config.batch_size,\n",
        "            num_workers=config.num_workers,\n",
        "        )\n",
        "\n",
        "        model.train()\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = time.time()\n",
        "        data_iter = iter(train_loader)\n",
        "        while True:\n",
        "\n",
        "            # fetch the next batch (x, y) and re-init iterator if needed\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(train_loader)\n",
        "                batch = next(data_iter)\n",
        "            #batch = [t.to(self.device) for t in batch]\n",
        "            x, y = batch\n",
        "\n",
        "            # forward the model\n",
        "            logits, self.loss = model(x, y)\n",
        "\n",
        "            # backprop and update the parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.trigger_callbacks('on_batch_end')\n",
        "            self.iter_num += 1\n",
        "            tnow = time.time()\n",
        "            self.iter_dt = tnow - self.iter_time\n",
        "            self.iter_time = tnow\n",
        "\n",
        "            # termination conditions\n",
        "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
        "                break\n",
        "\n",
        "model = GPT(config)\n",
        "trainer = Trainer(config, model, train_dataset)\n",
        "trainer = Trainer(config, model, train_dataset)\n",
        "\n",
        "def batch_end_callback(trainer):\n",
        "    if trainer.iter_num % 5 == 0:\n",
        "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "trainer.run()"
      ],
      "metadata": {
        "id": "CRWfL7ktFP47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'ou are all resolved rather to '\n",
        "sample_ids = torch.Tensor(enc.encode_ordinary(text)).long()\n",
        "sample_ids = torch.unsqueeze(sample_ids, 0)\n",
        "result = model.generate(sample_ids, max_new_tokens=50, temperature=1, do_sample=False, top_k=None)\n",
        "\n",
        "#print(enc.decode(result.detach().tolist()[0]))"
      ],
      "metadata": {
        "id": "BiF8WpHlFP7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_gpt.pth')"
      ],
      "metadata": {
        "id": "6R5Wi2StEMlg"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(enc.decode(result.detach().tolist()[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0RbNrp5qsaP",
        "outputId": "a2f27891-29b8-420f-c73d-db9b487a8eb8"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lord:\n",
            "Rise! My people, conquer the north!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}